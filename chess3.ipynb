{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a79485",
   "metadata": {},
   "source": [
    "### Description\n",
    "This workbook will train a chess AI. It will use a combination of first backwards propagation and then genetic algorithms.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494a75b9",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da341e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import chess.engine\n",
    "import chess\n",
    "import random\n",
    "from tensorflow.keras import datasets, layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e12f26",
   "metadata": {},
   "source": [
    "Before we create data we have to do some set up.  The plan is to have a 2d input each with a 13 hot vector.  (I chose 13 instead of 12 because I think probably will help teach the format of the board.  Of course you should leave one out in other applications because of the multicollinearity issue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8f8f888",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input: chess board objects\n",
    "#output 8,8,13 nparray\n",
    "def boardToInput(board):\n",
    "    #I should probably make this a np array but I will have to test if it is faster\n",
    "    pieces = ['.','r','n','b','q','k','p','P','R','N','B','Q','K']\n",
    "    #stringify the board\n",
    "    string = str(board)\n",
    "    #each col and row has a 13 1 hot vector of the current piece as set in the pieces variable\n",
    "    arr= np.zeros(shape=(8,8,13))\n",
    "    row = 0\n",
    "    col = 0    \n",
    "    #I know this is stupid we are doing this so many times I don't want to call len(string)\n",
    "    for indexer in range(127):\n",
    "        #Skip the spaces\n",
    "        if string[indexer] == ' ':\n",
    "            continue\n",
    "        #This means we are about to move on to a new col\n",
    "        if string[indexer] == '\\n':\n",
    "            col = 0\n",
    "            row = row + 1\n",
    "            continue        \n",
    "        arr[row][col][pieces.index(string[indexer])] = 1\n",
    "        col  = col + 1            \n",
    "    return arr\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0e3942",
   "metadata": {},
   "source": [
    "These methods will return np arrays of X and Y.  createData will have Y as the engines eval of the position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0348f7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick method to convert pychess evaluations to floats\n",
    "def valueToFloat(val):\n",
    "    #This is emperically the min and the max that I found for each.  Normalized theses\n",
    "    minEvaluation = -1500\n",
    "    maxEvaluation = 1500\n",
    "\n",
    "    string = str(val)\n",
    "    val = 0\n",
    "    \n",
    "    if '#' in string:\n",
    "        if string[1] == '+':\n",
    "            return 1\n",
    "        elif string[1] == '-':\n",
    "            return 0\n",
    "    val = float(string)\n",
    "    val = (val - minEvaluation)/(maxEvaluation-minEvaluation)\n",
    "    if val < 0:\n",
    "        val = 0\n",
    "    if val > 1:\n",
    "        val = 1\n",
    "    return val\n",
    "    #return val\n",
    "def createData(n,engine): \n",
    "    X = np.zeros((n,8,8,13))\n",
    "    Y = np.zeros(n)\n",
    "    isBlack = False\n",
    "    board = chess.Board()\n",
    "    for i in tqdm(range(n)):\n",
    "        isBlack = not isBlack\n",
    "        if board.outcome() != None: \n",
    "            board=chess.Board()\n",
    "            isBlack = False\n",
    "        val = valueToFloat(engine.analyse(board, chess.engine.Limit(depth=1))['score'].white())\n",
    "        X[i] = (boardToInput(board))\n",
    "        Y[i] = val\n",
    "        moves = list(board.legal_moves)\n",
    "        if random.random() > .1:\n",
    "            move = random.choice(moves)\n",
    "        else:\n",
    "            move =getMoveEngine(board,moves,isBlack)\n",
    "        board.push(move)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "\n",
    "def createDataAutoEncoder(n,engine): \n",
    "    X = np.zeros((n,8,8,13))\n",
    "    Y = np.zeros(n)\n",
    "    board = chess.Board()\n",
    "    for i in tqdm(range(n)):\n",
    "        if board.outcome() != None: \n",
    "            board=chess.Board()\n",
    "        val = valueToFloat(engine.analyse(board, chess.engine.Limit(depth=1))['score'].white())\n",
    "        X[i] = boardToInput(board)\n",
    "        Y[i] = val\n",
    "        moves = list(board.legal_moves)\n",
    "        move = random.choice(moves)\n",
    "        board.push(move)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def getMoveEngine(board,moves,isBlack):\n",
    "    bestEval = -5000\n",
    "    bestMove = moves[0]\n",
    "    for move in moves:\n",
    "        board.push(move)\n",
    "        val = valueToFloat(engine.analyse(board, chess.engine.Limit(depth=1))['score'].white())\n",
    "        if isBlack:\n",
    "            val =val*-1\n",
    "        if val > bestEval:\n",
    "            bestMove = move\n",
    "            bestEval = val\n",
    "        board.pop()\n",
    "    return bestMove\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c4fea8",
   "metadata": {},
   "source": [
    "Now we should actaully create the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "394cc05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = chess.engine.SimpleEngine.popen_uci(\".\\\\stockfish\\\\stockfish_15.1_win_x64_avx2\\\\stockfish.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c3cc0324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 1000000/1000000 [1:14:31<00:00, 223.64it/s]\n",
      " 98%|███████████████████████████████████████████████████████████████████████▍ | 195857/200000 [15:37<00:19, 209.01it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_size = 1000000\n",
    "testing_size = int(training_size*.2)\n",
    "Xtr, Ytr = createData(training_size,engine)\n",
    "Xte, Yte = createData(testing_size,engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "423ab7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(Ytr,bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da899bd7",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "To start we will train a basic model with these parameters:\n",
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9272ebd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_44 (Conv2D)          (None, 8, 8, 32)          3776      \n",
      "                                                                 \n",
      " conv2d_45 (Conv2D)          (None, 8, 8, 32)          9248      \n",
      "                                                                 \n",
      " conv2d_46 (Conv2D)          (None, 8, 8, 32)          9248      \n",
      "                                                                 \n",
      " conv2d_47 (Conv2D)          (None, 8, 8, 32)          9248      \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 64)                131136    \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 197,921\n",
      "Trainable params: 197,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Input(shape=(8, 8, 13)))\n",
    "#model.add(layers.Conv2D(64, (8, 3), activation='relu', input_shape=(8, 8, 13)))\n",
    "model.add(layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation=tf.nn.relu))\n",
    "model.add(layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(layers.Dense(64, activation=tf.nn.relu))\n",
    "model.add(layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(layers.Dense(64, activation=tf.nn.relu))\n",
    "model.add(layers.Dense(32, activation=tf.nn.relu))\n",
    "model.add(layers.Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a784c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0169"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 129s 1s/step - loss: 0.0169 - val_loss: 0.0140 - lr: 1.0000e-04\n",
      "Epoch 2/500\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0140"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 137s 2s/step - loss: 0.0140 - val_loss: 0.0139 - lr: 1.0000e-04\n",
      "Epoch 3/500\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0139"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 124s 1s/step - loss: 0.0139 - val_loss: 0.0138 - lr: 1.0000e-04\n",
      "Epoch 4/500\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0138"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 123s 1s/step - loss: 0.0138 - val_loss: 0.0136 - lr: 1.0000e-04\n",
      "Epoch 5/500\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0136"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 121s 1s/step - loss: 0.0136 - val_loss: 0.0135 - lr: 1.0000e-04\n",
      "Epoch 6/500\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0135"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 121s 1s/step - loss: 0.0135 - val_loss: 0.0134 - lr: 1.0000e-04\n",
      "Epoch 7/500\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0134"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 122s 1s/step - loss: 0.0134 - val_loss: 0.0133 - lr: 1.0000e-04\n",
      "Epoch 8/500\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0132"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 127s 1s/step - loss: 0.0132 - val_loss: 0.0131 - lr: 1.0000e-04\n",
      "Epoch 9/500\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0132"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 173s 2s/step - loss: 0.0132 - val_loss: 0.0130 - lr: 1.0000e-04\n",
      "Epoch 10/500\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0130"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 169s 2s/step - loss: 0.0130 - val_loss: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 11/500\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0129"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 152s 2s/step - loss: 0.0129 - val_loss: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 12/500\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0128"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 154s 2s/step - loss: 0.0128 - val_loss: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 13/500\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0127"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 156s 2s/step - loss: 0.0127 - val_loss: 0.0126 - lr: 1.0000e-04\n",
      "Epoch 14/500\n",
      "90/90 [==============================] - 156s 2s/step - loss: 0.0127 - val_loss: 0.0129 - lr: 1.0000e-04\n",
      "Epoch 15/500\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0127"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 143s 2s/step - loss: 0.0127 - val_loss: 0.0125 - lr: 1.0000e-04\n",
      "Epoch 16/500\n",
      "90/90 [==============================] - 144s 2s/step - loss: 0.0125 - val_loss: 0.0128 - lr: 1.0000e-04\n",
      "Epoch 17/500\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 153s 2s/step - loss: 0.0125 - val_loss: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 18/500\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0124"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 154s 2s/step - loss: 0.0124 - val_loss: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 19/500\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0123"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 156s 2s/step - loss: 0.0123 - val_loss: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 20/500\n",
      "90/90 [==============================] - 151s 2s/step - loss: 0.0123 - val_loss: 0.0124 - lr: 1.0000e-04\n",
      "Epoch 21/500\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0123"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 155s 2s/step - loss: 0.0123 - val_loss: 0.0123 - lr: 1.0000e-04\n",
      "Epoch 22/500\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0122"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 156s 2s/step - loss: 0.0122 - val_loss: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 23/500\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0121"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 177s 2s/step - loss: 0.0121 - val_loss: 0.0121 - lr: 1.0000e-04\n",
      "Epoch 24/500\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0121"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow.keras.callbacks as callbacks\n",
    "\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "#model.compile(optimizer=optimizers.Adam(1e-2), loss=tf.keras.losses.BinaryCrossentropy(from_logits=False))\n",
    "model.compile(optimizer=optimizers.Adam(1e-4), loss='mean_squared_error')\n",
    "\n",
    "#model.summary()\n",
    "checkpoint_filepath = '/tmp/checkpoint/'\n",
    "'''\n",
    "model_checkpointing_callback = ModelCheckpoint(\n",
    "    filepath = checkpoint_filepath,\n",
    "    save_best_only= True,\n",
    ")\n",
    "'''\n",
    "model_checkpointing_callback = ModelCheckpoint(\n",
    "    filepath = checkpoint_filepath,\n",
    "    save_best_only= True,\n",
    ")\n",
    "\n",
    "model.fit(Xtr, Ytr,\n",
    "          batch_size=10000,\n",
    "          epochs=500,\n",
    "          verbose=1,\n",
    "          validation_split=0.1,\n",
    "            callbacks=[callbacks.ReduceLROnPlateau(monitor='loss', patience=10),\n",
    "                     callbacks.EarlyStopping(monitor='loss', patience=15, min_delta=1e-4),model_checkpointing_callback])\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "59e01ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4980052760000001"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(Ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e96f0374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.4970151]], dtype=float32)>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board = chess.Board()\n",
    "testInput = np.array([boardToInput(board)])\n",
    "model(testInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4be2621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMoveTensor(board,moves,model,isBlack):\n",
    "    bestEval = -2\n",
    "    bestMove = None\n",
    "    for move in moves:\n",
    "        board.push(move)\n",
    "        val = model(np.array([boardToInput(board)]))\n",
    "        if isBlack:\n",
    "            val = 1-val\n",
    "        if val > bestEval:\n",
    "            bestMove = move\n",
    "            bestEval = val\n",
    "        board.pop()\n",
    "    return bestEval,bestMove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "073628e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "board =chess.Board()\n",
    "isBlack = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6b1c3e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ". r . . k b n r\n",
      "p . p n p p p p\n",
      ". . . . . . . .\n",
      ". . . . . . . .\n",
      ". . P . . . . .\n",
      ". . B . . . . .\n",
      "P P . . . P B P\n",
      "R N . . K . . R\n"
     ]
    }
   ],
   "source": [
    "\n",
    "isBlack = not isBlack\n",
    "print()\n",
    "\n",
    "moves = list(board.legal_moves)\n",
    "engineMove = getMoveEngine(board,moves,isBlack)\n",
    "evaluation, AImove =  getMoveTensor(board,moves,model,isBlack)\n",
    "if isBlack:\n",
    "    move = AImove\n",
    "else:\n",
    "    move = AImove\n",
    "\n",
    "board.push(move)\n",
    "'''\n",
    "print(\"Model says: \" +str(float(evaluation)))\n",
    "if isBlack:\n",
    "    print(\"Engine evaluates it at: \" + str(valueToFloat(engine.analyse(board, chess.engine.Limit(depth=1))['score'].white())))\n",
    "else:\n",
    "    print(\"Engine evaluates it at: \" + str(valueToFloat(engine.analyse(board, chess.engine.Limit(depth=1))['score'].white())))\n",
    "'''\n",
    "print(board)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f62641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a233b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
